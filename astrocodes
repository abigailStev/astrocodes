#!/usr/bin/env python

import os
import json
import sys
import argparse
from collections import OrderedDict
from pprint import pprint
import glob
from datetime import datetime
import time

import requests
from bs4 import BeautifulSoup
from jinja2 import Template

DATA_DIR = "data"
STATIC_DIR = "static"
OUTPUT_DIR = "output"

# PYPI_CLASSIFIER_URL: To get the URL for a given classifier, go to
# PyPI's "browse" page (https://pypi.python.org/pypi?%3Aaction=browse),
# browse to the classifier you want, and copy the URL.
PYPI_MAIN_URL = "https://pypi.python.org/pypi?%3Aaction=index"
PYPI_CLASSIFIER_URL = ("https://pypi.python.org/pypi"
                       "?:action=browse&show=all&c=387")
PYPI_PACKAGE_URL = "https://pypi.python.org/pypi/{0}/json"
PYPI_DATA_DIR = os.path.join(DATA_DIR, "pypi")

ASCL_URL = "http://ascl.net/code/json"
ASCL_FILE = os.path.join(DATA_DIR, "ascl.json")

def info(msg):
    """Print a line in blue (assumes we have a color terminal!)"""
    print("\033[1m\033[34m", "INFO:", msg, "\033[0m")


def urlretrieve(url, fname):
    """ Retrieve URL and write to a file.

    Parameters
    ----------
    url : str
        Specific URL to get using the 'requests' package.
    fname : str
        Name of file to write URLs to.

    """
    info("Fetching " + url)
    r = requests.get(url)
    with open(os.path.join(fname), "w") as f:
        f.write(r.text)


def read_json(fname):
    """ Reads from file into JSON. """
    with open(fname, "r") as f:
        return json.load(f)


def write_json(d, fname):
    """ Writes JSON to file. """
    with open(fname, "w") as f:
        json.dump(d, f)


def parse_pypi_html(html_doc, terms=None, strip_version=False):
    """Parse HTML page with package listing from PyPI.

    Parameters
    ----------
    html_doc : str
        HTML document.

    terms : list of str, optional
        If given, only include a package in results if its description includes
        one or more of these terms.

    strip_version : bool, optional
        If True, expect the package name to include a version number and
        strip it off.

    Returns
    -------
    packages : dict
        Dictionary where keys are package names and values are descriptions.
    """

    soup = BeautifulSoup(html_doc)
    result = {}

    # main table on page has class 'list'. Get table data elements from it.
    for row in soup.select("table[class~=list] tr"):
        cells = row.select("td")

        # Header row has no <td>'s and last row has one <td>. Skip these.
        if len(cells) != 2:
            continue

        # first column has an <a href...> element containing the package name.
        name = cells[0].a.string
        description = cells[1].string
        if strip_version:
            name = name.split('\xa0')[0]

        # crop duplicates
        if name in result:
            continue

        # search terms
        if terms is None:
            result[name] = description
        elif description is not None:
            for term in terms:
                if term in description.lower():
                    result[name] = description
                    break

    return result


def fetch_pypi():
    """ Fetch 'astronomy' packages listed on PyPI. """
    if not os.path.exists(PYPI_DATA_DIR):
        os.makedirs(PYPI_DATA_DIR)

    # Get names and descriptions of all "astronomy" packages.
    fname = os.path.join(PYPI_DATA_DIR, "packages.json")
    if not os.path.exists(fname):
        info("Fetching " + PYPI_CLASSIFIER_URL)
        r = requests.get(PYPI_CLASSIFIER_URL)
        info("Parsing " + PYPI_CLASSIFIER_URL)
        pkgs = parse_pypi_html(r.text)

        info("Fetching " + PYPI_MAIN_URL)
        r = requests.get(PYPI_MAIN_URL)
        info("Parsing " + PYPI_MAIN_URL)
        pkgs2 = parse_pypi_html(r.text, terms=["astro", "cosmo"],
                                strip_version=True)
        pkgs.update(pkgs2)

        write_json(pkgs, fname)
    else:
        pkgs = read_json(fname)

    # Get and save data for each package
    for name in pkgs:
        url = PYPI_PACKAGE_URL.format(name)
        fname = os.path.join(PYPI_DATA_DIR, name + ".json")
        if not os.path.exists(fname):
            urlretrieve(url, fname)


def is_url_a_repo(url_string):
    """ Check if a given URL string is a GitHub or BitBucket repository URL
    (hopefully, the source code).

    TODO: Check 'github.com/*/*' and 'bitbucket.org/*/*', so that you know the \
    URL is directly to the repository directory.

    Parameters
    ----------
    url_string : str
        A URL scraped from href HTML tags on a website.

    Returns
    -------
    bool
        True if the URL is to a repository, False if it is not.
    """
    url_string = url_string.lower()
    if "github.com" in url_string or "bitbucket.org" in url_string:
        print("URL is a repo! %s" % url_string)
        return True

    print("URL is not a repo. %s" % url_string)
    return False


def parse_html_for_urls(html_doc):
    """ Parse the sourcepage HTML for URLs, and return a list of all URLs.

    Parameters
    ----------
    html_doc : str
        HTML document.

    Returns
    -------
    list of str
        URLs listed in 'a href' in the HTML document.
    """
    soup = BeautifulSoup(html_doc)
    urls = []

    # Scrape page for 'a href' links
    for link in soup.find_all('a'):

        # You found a link!
        print(link.get('href'))
        temp_link = link.get('href')

        # Crop duplicates
        if temp_link in urls:
            continue

        # Append unique link to the URL list
        urls.append(temp_link)

    print(urls)
    return urls


def check_sourcecode_url(source_url):
    """ Get the URL of the source code repository.
    URLs from the source webpage listed on PyPI or ASCL.

    Parameters
    ----------
     source_url : str
        The URL listed as the source.

    Returns
    -------
    str
        The URL of the source code's repository. Returns 'None' if no source
        code repository URL is found.
    """

    if is_url_a_repo(source_url):
        return source_url

    source_html = requests.get(source_url)
    urls = parse_html_for_urls(source_html.text)

    for link in urls:
        if is_url_a_repo(link):
            return link

    return ""


# TODO: use regex's
_LANG_KEYPHRASES = {"python": "Python",
                    "Python": "Python",
                    "Java": "Java",
                    "java": "Java",
                    "in C ": "C",
                    "in C,": "C",
                    "in C.": "C",
                    "C++": "C++",
                    "IDL": "IDL",
                    "FORTRAN": "Fortran",
                    "Fortran": "Fortran",
                    "fortran": "Fortran"}

def detect_language(s):
    """Stupid detection of programming language from a ASCL description"""

    for keyphrase, lang in _LANG_KEYPHRASES.items():
        if keyphrase in s:
            return lang

    return None

def get_source_urls():
    """ Gets source URLs from PyPI and ASCL for rendering the table. """

    """ Current error:
    Traceback (most recent call last):
  File "./astrocodes", line 396, in <module>
    render()
  File "./astrocodes", line 321, in render
    temp_link = check_sourcecode_url(url)
  File "./astrocodes", line 238, in check_sourcecode_url
    if is_url_a_repo(link):
  File "./astrocodes", line 171, in is_url_a_repo
    url_string = url_string.lower()
AttributeError: 'NoneType' object has no attribute 'lower'
"""

    # TODO: Need to get list of source URLs for the packages/programs

    # Probably want to call
    for url in pkg["urls"]:
        temp_link = check_sourcecode_url(url)
        print(temp_link)

    # TODO: Return URLs? Save back to list or dict?

def render():

    # Load PyPI packages
    fnames = os.listdir(PYPI_DATA_DIR)
    fnames.remove("packages.json")
    pypi_pkgs = [read_json(os.path.join(PYPI_DATA_DIR, fname))
                 for fname in fnames]
    pypi_pkgs = {pkg["info"]["name"].lower(): pkg for pkg in pypi_pkgs}

    # Load ASCL packages
    ascl_pkgs = read_json(ASCL_FILE)
    ascl_pkgs = {pkg["title"].split(":")[0].lower(): pkg
                 for pkg in ascl_pkgs.values()}  # re-key packages.

    # clean empty links
    for pkg in ascl_pkgs.values():
        while '' in pkg["site_list"]:
            pkg["site_list"].remove('')

    # get joint names
    names = set(pypi_pkgs)
    names.update(ascl_pkgs)

    # combine packages
    packages = []
    for name in names:
        pkg = {"name": name,
               "urls": []}

        if name in pypi_pkgs:
            pypi_pkg = pypi_pkgs[name]
            pkg["pypi_url"] = pypi_pkg['info']['package_url']
            pkg["language"] = "Python"
            pkg["language_from"] = "because package listed on PyPI"
            if pypi_pkg["info"]["home_page"] is not None:
                pkg["urls"].append(pypi_pkg["info"]["home_page"])
            if pypi_pkg["info"]["docs_url"] is not None:
                pkg["urls"].append(pypi_pkg["info"]["docs_url"])

        if name in ascl_pkgs:
            ascl_pkg = ascl_pkgs[name]
            pkg["ascl_url"] = 'http://ascl.net/' + ascl_pkg['ascl_id']
            pkg["urls"].extend(ascl_pkg["site_list"])
            pkg["description"] = ascl_pkg["abstract"]
            if "language" not in pkg:
                lang = detect_language(ascl_pkg["abstract"])
                if lang is not None:
                    pkg["language"] = lang
                    pkg["language_from"] = "based on ASCL abstract"
        packages.append(pkg)

    # sort by name
    packages.sort(key=lambda x: x["name"])

    # load HTML template
    with open(os.path.join(STATIC_DIR, "template.html")) as f:
        template_html = f.read()
    template = Template(template_html)

    # copy static files
    for subdir in ["css", "js", "style"]:
        src = os.path.join(STATIC_DIR, subdir)
        dst = os.path.join(OUTPUT_DIR, subdir)
        if not os.path.exists(dst):
            shutil.copytree(src, dst)

    # render and ouput page
    ctime = time.strftime('%Y %b %d %l:%M %p %Z')
    page = template.render(packages=packages,
                           last_generated=ctime)
    with open(os.path.join(OUTPUT_DIR, "index.html"), "w") as f:
        f.write(page)

    """
    n = 0
    for name in pkgs.keys():
        if n > 10:
            break
        n += 1

        print()
        print(name)

        print("ASCL site_list:", end=None)
        if "ascl" in pkgs[name]:
            print(pkgs[name]["ascl"]["site_list"])

        print("PyPI home_page:", end=None)
        if "pypi" in pkgs[name]:
            print(pkgs[name]["pypi"]["info"]["home_page"])

        print("PyPI docs_url:", end=None)
        if "pypi" in pkgs[name]:
            print(pkgs[name]["pypi"]["info"]["docs_url"])
    """

    # TODO
    #
    # fill 'home_url', 'docs_url', 'repo_url' from
    # - ASCL site_list
    # - PyPI docs_url, home_page

    # fill 'summary' from pypi summary and ASCL title

    # Link-check ASCL URLs with HEAD requests - cut if all dead.

    # Try to fill in missing repos by getting homepage index page and scanning
    # links.

    # Get stats from github



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Fetch the data.")
    parser.add_argument("action", help="What to do?")
    args = parser.parse_args()

    if args.action == "fetch-pypi":
        fetch_pypi()
    elif args.action == "fetch-ascl":
        urlretrieve(ASCL_URL, ASCL_FILE)
    elif args.action == "render":
        render()
